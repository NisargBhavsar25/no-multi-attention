# Model Configuration
model:
  type: "bert"
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 2
  vocab_size: 30522  # BERT vocab size

# Training Configuration
training:
  batch_size: 32
  learning_rate: 2e-5
  num_train_epochs: 3
  warmup_steps: 500
  weight_decay: 0.01
  max_seq_length: 512
  gradient_accumulation_steps: 1
  fp16: false

# Dataset Configuration
dataset:
  name: "imdb"
  train_file: "data/train.csv"
  test_file: "data/test.csv"
  cache_dir: "data/cache"

# Evaluation Configuration
evaluation:
  batch_size: 64
  metrics:
    - "accuracy"
    - "latency"
    - "memory_usage"

# Logging Configuration
logging:
  log_dir: "experiments/logs"
  save_steps: 1000
  eval_steps: 500
  save_total_limit: 2 