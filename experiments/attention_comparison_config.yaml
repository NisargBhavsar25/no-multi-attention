# Model Configuration
model:
  type: "bert"
  hidden_size: 384  # Reduced from 768 for faster training
  num_hidden_layers: 3  # Reduced from 6 for faster training
  num_attention_heads: 6  # Reduced from 12 for faster training
  intermediate_size: 1536  # Reduced from 3072 for faster training
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 2
  vocab_size: 30522  # BERT vocab size

# Training Configuration
training:
  batch_size: 16
  learning_rate: 0.00002
  num_train_epochs: 3
  warmup_steps: 500
  weight_decay: 0.01
  max_seq_length: 128
  gradient_accumulation_steps: 1
  fp16: false

# Dataset Configuration
dataset:
  name: "imdb"
  train_file: "data/train.csv"
  test_file: "data/test.csv"
  cache_dir: "data/cache"
  num_train_samples: 100  # Number of training samples to use (-1 for all samples)
  num_test_samples: 100   # Number of test samples to use (-1 for all samples)

# Evaluation Configuration
evaluation:
  batch_size: 32
  metrics:
    - "accuracy"
    - "latency"
    - "memory_usage"

# Logging Configuration
logging:
  log_dir: "experiments/results/attention_comparison_full"
  save_steps: 1000  # Save less frequently for full run
  eval_steps: 200  # Evaluate metrics less frequently for full run
  save_total_limit: 2

# Experiment Configuration
experiment:
  name: "attention_comparison_full"  # Changed to indicate full experiment
  description: "Comparison of standard, inhibitor, quadratic inhibitor, consmax, approx_exp, standard with ReLU activation, and quadratic inhibitor with ReLU activation"
  attention_types:
    - "standard"
    - "inhibitor"
    - "quadratic_inhibitor"
    - "consmax"
    - "approx_exp"
  use_relu_activation: true  # Enable standard attention with ReLU activation
  use_quadratic_inhibitor_relu: true  # Enable quadratic inhibitor attention with ReLU activation 